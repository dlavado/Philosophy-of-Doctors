group:
  value: 'MNIST'
output_dir: 
  value: 'experiments/admm/outputs'
# ------------------ #
# dataset config
# ------------------ #
dataset:
  value: 'mnist'
data_path:
  value: ../datasets/mnist/
batch_size:
  value: 32
num_workers:
  value: 8
# ------------------ #
# model config
# ------------------ #
model:
  value: 'cnn'
kernel_size:
  value: 3
hidden_dim:
  value: 128
in_channels:
  value: 1
# ------------------ #
# training config
# ------------------ #
optimizer:
  value: 'adam' #'adam' 
learning_rate:
  value: 0.01
max_epochs:
  value: -1 # -1 for infinite
gpus:
  value: -1 # -1 for all available gpus
early_stop_metric:
  value: 'val_loss'

# ------------------ #
# Lit Trainer config
# ------------------ #
fast_dev_run:
  value: True
precision: # 16 or 32 FPU precision
  value: 16
  description: 'FPU precision'
auto_lr_find:
  value: True
auto_scale_batch_size:
  value: False
profiler:
  value: False
  description: 'PyTorch Lightning profiler'
accumulate_grad_batches:
  value: None
  description: 'Accumulate gradients on k batches before performing a backward pass'
save_onnx:
  value: False
  description: 'Save model in onnx format'