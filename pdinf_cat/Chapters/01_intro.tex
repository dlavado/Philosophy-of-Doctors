%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter1.tex
%% NOVA thesis document file
%%
%% Chapter with introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE 01_intro.tex}%

\chapter{Introduction}\label{cha:introduction}

% Main problems I was to address:

%1. Domain Challenge: Power Grid Inspections Are Under-Explored and High-Risk
% Despite the rapid growth of 3D scene understanding in domains such as autonomous driving, indoor mapping, and VR/AR, 
% critical infrastructure monitoring—such as power grid inspection—remains severely underexplored. 
% Public datasets and research contributions in this area are scarce, creating a significant gap between 
% state-of-the-art (SOTA) research and the needs of real-world applications.

% Today, most power grid inspections are conducted manually, involving daily coverage of thousands of kilometers by human operators. 
% This approach is not only time-consuming and costly, but also susceptible to human error and latency, potentially overlooking small 
% defects that may lead to catastrophic failures, including power outages, equipment damage, and even forest fires. There is a pressing 
% need for more reliable, automated, and scalable 3D analysis tools tailored to these safety-critical environments.

% 2. Methodological Challenge: The Cost of Scaling in 3D Scene Understanding
% In parallel, 3D scene understanding research has witnessed a trend toward scaling models and datasets to achieve performance gains. 
% However, this strategy is reaching diminishing returns. For example, Point Transformer V2 (PTV2) triples the number of parameters compared to PTV1, 
% yet achieves only a marginal 2% performance increase on benchmarks such as SemanticKITTI. 
% Meanwhile, PTV3 relies on access to diverse and rich 3D data, which is often unavailable in many industrial scenarios.
% This trend raises a fundamental question: Can we achieve better performance without simply increasing data or model size?

% 3. Structural Challenge: Lack of Inductive Biases in 3D Models
% A key reason for the success of CNNs in 2D image processing is their reliance on strong inductive biases.
% Images are structured in grids, enabling local correlations and translation-equivariant feature extraction. 
% However, 3D point clouds lack this regular structure—they are unordered, sparse, and irregular. 
% As a result, 3D convolutional kernels struggle to generalize as effectively. Yet, 3D data inherently contains rich geometric relationships, 
% such as local distances, orientations, and curvatures. These can be leveraged as geometric inductive biases to inform how features
% are aggregated and processed, enabling better generalization and more interpretable models. 
% This insight opens the door to more efficient, structure-aware models that can work robustly even in data-scarce or resource-constrained settings.

%%%% CONTEXT

3D scene understanding is a fast-moving field at the intersection of computer vision
and spatial perception, with applications spanning autonomous driving, robotics, and
augmented or mixed reality. At its core, the task involves interpreting 3D data,
be it point clouds, meshes, or volumetric representations, to extract meaningful
insights about the environment.
%
The central problems tackled in this space include semantic segmentation,
object detection, and scene reconstruction. In semantic segmentation, the goal
is to assign a semantic label to each point or region in a 3D scene. Object
detection focuses on identifying and localizing distinct objects, while scene
reconstruction aims to build a structured 3D representation from unstructured
inputs like point clouds or raw sensor data.
%

While traditional signal and image processing have laid strong foundations,
introducing a third spatial dimension brings both unique advantages and
non-trivial challenges. On the upside, 3D data captures spatial geometry and
structural information more faithfully than 2D projectionsm enabling accurate
depth perception, size and shape estimation, and precise spatial localization,
all of which are crucial in dynamic environments such as traffic scenes.
%
Moreover, 3D data is less susceptible to environmental conditions like lighting
changes or motion blur, which often degrade 2D image quality.
%
However, the benefits of 3D come with substantial complexity. Point clouds,
despite being one of the most common formats, lack regular structure, suffer
from non-uniform sampling, and are permutation invariant, making them difficult
to process with standard convolutional architectures that thrive on grid-like
data. In turn, volumetric representations impose structure through voxelization
but suffer from extreme memory and compute costs, especially in large-scale
scenes, due to their cubic scaling.
%
Meshes introduce yet another layer of complexity, with irregular topologies
that require expensive operations like remeshing or normal estimation and
customized convolutions on graph-like structures.
%
Despite these obstacles, the field has progressed rapidly over the past decade.
We have seen the emergence of powerful deep learning models, from the seminal
PointNet~\cite{qi2017pointnet} to the recent Point Transformer
V3~\cite{wu2023ptv3}, alongside high-quality annotated datasets like
SemanticKITTI~\cite{behley2019semantickitti}, ScanNet~\cite{dai2017scannet},
and NuScenes~\cite{caesar2020nuscenes} that have become standard evaluation
benchmarks.
%
However, a recent trend is beginning to raise questions. While models continue
to grow in size and scale --- for example, Point Transformer
V3~\cite{wu2023ptv3} nearly quadruples the parameter count of its predecessor
(from 12.8M to 46.2M) --- the performance gains are marginal: improvements of
just 1–3\% in mean IoU across major
benchmarks~\cite{behley2019semantickitti,dai2017scannet,caesar2020nuscenes}.
These models also depend on resource-heavy training pipelines, such as
multi-dataset point prompt training (PPT)~\cite{wu2024towards}, which demand
access to diverse datasets and large-scale compute infrastructure.

This pattern poses a critical question: \textit{can we design better-performing
    models without simply scaling up data and model size?}

To ground this, consider the paradigm shift that occurred in 2D vision with the
rise of convolutional neural networks (CNNs). CNNs outperformed multi-layer
perceptrons (MLPs) not by being more expressive in theory, MLPs are universal
function approximators, but by building in strong inductive biases.
%
CNNs take advantage of local correlations and translation-equivariant feature
extraction. That is, they assume that local regions matter, and that the same
features can occur anywhere in the input. This results better parameter
efficiency, improved generalization, and enhanced explainability.
%
These assumptions, tailored to the structure of image data, are exactly what
MLPs lack: treat all inputs independently and equally and lack structure
awareness.
%
% An inductive bias is a built-in assumption that a model makes about the nature
% of the data. It helps guide the learning process by narrowing down the set of
% possible solutions the model considers and allows the model to generalize
% better from limited training data.

In 3D scene understanding, early approaches tried to force 3D data into 2D or
grid-like formats to reuse well-established operations like convolutions.
%
Multi-view approaches projected point clouds into 2D images from multiple
viewpoints~\cite{su2015multi,lawin2017deep,feng2018gvcnn}, allowing the use of
2D CNNs.
%
Volumetric
methods~\cite{maturana2015voxnet,zhang2020polarnet,chen2023voxelnext}
discretized the space into voxels and applied 3D convolutions. But both
approaches introduce information loss or computational bottlenecks.
%
This led to a new wave of point-based
methods~\cite{qi2017pointnet,qi2017pointnet++,li2018pointcnn,thomas2019kpconv,hu2020randla,kong2023rethinking,qian2022pointnext},
which process raw point clouds directly. Some recovered convolutional structure
through operations like KPConv~\cite{thomas2019kpconv}, which attempt to
restore the benefits of local aggregation.
%
Still, a major difficulty remains: unlike pixels in an image, points do not
come with a fixed neighborhood structure. This makes learning local patterns
challenging and inconsistent.

Recently, attention-based models emerged as the new frontier. Inspired by the
Transformer architecture~\cite{vaswani2017attention}, recent works adapted
self-attention to 3D data, achieving strong benchmark
performance~\cite{zhao2021point,wu2022point,lai2022stratified,park2022fast,wang2023octformer,lai2023spherical,wu2023ptv3}.
But attention mechanisms, like MLPs, lack inductive biases, treating all inputs
uniformly without any geometric assumptions.
%
This brings us to a key insight: \textbf{3D data is inherently geometric}. It
encodes local distances, angles, orientations, and curvature, all of which are
lost in 2D projections. Yet current state-of-the-art models often ignore this
rich structure in favor of generic function approximation.

%
In this thesis, we explore a different path. We argue that \textbf{geometric
    inductive biases}, the kind of domain-specific assumptions that powered CNNs in
2D vision, can and should be brought into 3D scene understanding. By explicitly
modeling local geometric relationships, we aim to guide feature learning in a
way that improves generalization, reduces reliance on massive datasets, and
yields more interpretable and efficient models.

In parallel to this, while much of the academic focus in 3D scene understanding
has centered around benchmark-rich domains like autonomous driving or indoor
mapping, critical real-world applications remain vastly underexplored.
%
A prime example is power grid inspection, a safety critical task where the gap
between state-of-the-art research and industrical needs is particularly acute.
%
Despite the maturity of 3D processing techniques, power grid monitoring still
relies heavily on manual inspection, requiring human operators to inspect
thousands of kilometers each day. This method is not only inefficient and
error-prone but also poses serious risks, such as missed defects that can
escalate into power outages, infrastructure damage, or even wildfires.
%
In this work, we argue that integration of geometric inductive biases is not
just an academic endeavor, but a practical necessity. By embedding these
biases into 3D feature extraction pipelines, we can unlock the potential for
more reliable, automated, and scalable 3D analysis tools tailored to
safety-critical environments like power grid inspection.

\section{Main Thesis Challenges}

\section{Proposed Approach}

\section{Expected Contributions}

\section{Document Structure}